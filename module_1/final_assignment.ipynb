{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final assignment\n",
    "\n",
    "The final exercise involves converting data from one or more providers.\n",
    "\n",
    "Since this exercise is designed to prepare you for real project work, the information you need to solve it might be slightly incomplete or not provided in context. Use your best judgment!\n",
    "\n",
    "Parts of this assignment can be solved in several ways. Use descriptive variable names and comments or descriptive text if necessary to clarify.\n",
    "The final solution should be clear to your colleagues and will be shared with some of your fellow students for review.\n",
    "\n",
    "The data will be used for MIKE modelling and must be converted to Dfs with **apppropriate EUM types/units** in order to be used by the MIKE software.\n",
    "\n",
    "The data is provided as a [zip file](https://github.com/DHI/getting-started-with-mikeio/raw/main/mini_book/data/stream_data.zip) and a [NetCDF file](https://github.com/DHI/getting-started-with-mikeio/raw/main/mini_book/data/ERA5_DutchCoast.nc)  (in the data folder - see FA.3 below).\n",
    "\n",
    "Inside the zip file, there are a many timeseries (ASCII format) of discharge data from streams located across several regions (`*.dat`).\n",
    "\n",
    "Static data for each region is found in a separate file (`region_info.csv`)\n",
    "\n",
    "![](../images/region_info.png)\n",
    "\n",
    "Pandas `read_csv` is very powerful, but here are a few things to keep in mind\n",
    "\n",
    "* Column separator e.g. comma (,)\n",
    "* Blank lines\n",
    "* Comments\n",
    "* Missing values\n",
    "* Date format\n",
    "\n",
    "The MIKE engine can not handle missing values / delete values, **fill in missing values** with interpolated values.\n",
    "\n",
    "In order to save diskspace, **crop the timeseries** to simulation period Feb 1 - June 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FA.1 Convert all timeseries to Dfs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mikeio\n",
    "\n",
    "from mikeio import Dataset\n",
    "from mikeio.eum import EUMType, ItemInfo, EUMUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is one way to find and filter filenames in a directory\n",
    "# [x for x in os.listdir(\"datafolder\") if \"some_str\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is useful!\n",
    "# help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of reading csv\n",
    "# df = pd.read_csv(\"../data/oceandata.csv\", comment='#', index_col=0, sep=',', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Convert all timeseries to dfs0 (remember that the notebook should be runnable for your peers so put the files somewhere reasonable). \n",
    "\n",
    "b) Read s15_east_novayork_river.dfs0, print the \"header\", plot, and show that the number of missing values is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "path_in = r\".\\stream_data\"\n",
    "timeseries = [x for x in os.listdir(path_in) if x.endswith(\".dat\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timeserie in timeseries:\n",
    "    df = pd.read_csv(os.path.join(path_in, timeserie), comment='%', index_col=0, sep='\\t', parse_dates=True)\n",
    "    ds = mikeio.from_pandas(df)\n",
    "    # os.makedirs(r'.\\results')\n",
    "    ds.to_dfs(rf'.\\results\\{timeserie[0:3]}.dfs0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FA.2 Add region specific info to normalize timeseries with surface area\n",
    "\n",
    "Each timeseries belongs to a region identified in the filename, e.g. `s15_east_novayork_river.dat` is located in the `novayork` region.\n",
    "\n",
    "a) Convert all timeseries to dfs0 with specific discharge, by doing: \n",
    "\n",
    "For each timeseries in the dataset:\n",
    "* Find out which region it belongs to (hint: the string method [split()](https://docs.python.org/3/library/stdtypes.html#str.split) will be useful)\n",
    "* Divide the timeseries values with the surface area for the region (take into account units)\n",
    "* Create a dfs0 file with specific discharge *(discharge / area)* (like the one with discharge from FA.1)\n",
    "\n",
    "b) Determine which station has the largest max specific discharge (in the simulation period).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FA.3 Gridded data\n",
    "\n",
    "ERA5 wave model for a part of the North Sea is available in the file `\"../data/ERA5_DutchCoast.nc\"`\n",
    "\n",
    "The dataset is provided in NetCDF\n",
    "\n",
    "These are the variables of interest:\n",
    "\n",
    "* Significant wave height (*Significant height of combined wind waves and swell*)\n",
    "* Mean wave direction\n",
    "\n",
    "Info about spatial and temporal axes can be found in the NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import mikeio\n",
    "import numpy as np\n",
    "\n",
    "ds = xr.open_dataset(\"../data/ERA5_DutchCoast.nc\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = ds.longitude\n",
    "lats_rev = ds.latitude[::-1] # reversed\n",
    "\n",
    "# geometry = mikeio.Grid2D(x=...,y=..., projection=\"LONG/LAT\")\n",
    "#hm0 = mikeio.DataArray(np.flip(...,axis=1),geometry=geometry, time=ds.time, item=mikeio.ItemInfo(...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mds = mikeio.Dataset([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Find the relevant variables in the NetCDF file.\n",
    "\n",
    "b) Create MIKE IO DataArrays for each variable\n",
    "\n",
    "c) Combine the arrays into a MIKE IO Dataset and write it to a dfs2 file \n",
    "\n",
    "d) Read the data from the file and document that the temporal max at (50N, 1E) of the significant wave height is the same in the dfs2 file as in the NetCDF file (note: due to rounding errors the may not be exactly the same)\n",
    "\n",
    "The file should look like this in MIKE Zero:\n",
    "![](../images/waves.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission of solution\n",
    "\n",
    "Your solution to the above tasks is to be delivered in the format of a single Jupyter notebook file. Please create a new and name it final_assignment_xyz.ipynb where xyz is your initials. It should be easy to understand and runnable by your peers.\n",
    "\n",
    "The solution will be reviewed by some of your fellow students as well as an instructor, which will provide feedback on both the correctnes and clarity of your solution.\n",
    "\n",
    "The submission and review process is handled by Academy + Eduflow.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa576ebcd40e010bdc0ae86b06ce09151f3424f9e9aed6893ff04f39a9299d89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
